o1 Preview

¡Claro! Puedo añadir una interfaz gráfica de usuario (GUI) amigable y con buena experiencia de usuario (UX) para facilitar el uso de la aplicación. Utilizaré PyQt5, una biblioteca popular para crear interfaces gráficas en Python.

A continuación te proporcionaré el código completo con la GUI integrada, junto con instrucciones detalladas sobre cómo instalar las dependencias y utilizar la aplicación.
Código con Interfaz Gráfica de Usuario

python

Execute

Copy Code
import os
import subprocess
import torch
import numpy as np
import locale
import shutil
import logging
from datetime import datetime
from pydub import AudioSegment
import whisper
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import soundfile as sf
from TTS.api import TTS
import sys

from PyQt5.QtWidgets import (
  QApplication, QWidget, QLabel, QLineEdit, QPushButton, QFileDialog,
  QHBoxLayout, QVBoxLayout, QMessageBox, QProgressBar, QComboBox
)
from PyQt5.QtCore import Qt, QThread, pyqtSignal

# Configuración de logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Función para obtener la codificación preferida
def getpreferredencoding(do_setlocale=True):
  return "UTF-8"

locale.getpreferredencoding = getpreferredencoding

# Clase para el procesamiento en segundo plano
class VideoProcessingThread(QThread):
  progress = pyqtSignal(int)
  finished = pyqtSignal(str)
  error = pyqtSignal(str)

  def __init__(self, input_video_path, src_language, target_language):
      super().__init__()
      self.input_video_path = input_video_path
      self.src_language = src_language
      self.target_language = target_language

  def run(self):
      temp_files = []
      try:
          if not os.path.exists(self.input_video_path):
              raise Exception("El archivo de video especificado no existe.")

          # Paso 1: Extraer audio
          audio_path = extract_audio(self.input_video_path)
          if not audio_path:
              raise Exception("Fallo en la extracción de audio.")
          temp_files.append(audio_path)
          self.progress.emit(10)

          # Paso 2: Reducir ruido
          cleaned_audio_file = remove_background_noise(audio_path)
          if not cleaned_audio_file:
              raise Exception("Fallo en la reducción de ruido.")
          temp_files.append(cleaned_audio_file)
          self.progress.emit(30)

          # Paso 3: Transcribir audio
          vocals_text = transcribe_audio_whisper(cleaned_audio_file, self.src_language)
          if not vocals_text:
              raise Exception("Fallo en la transcripción de audio.")
          self.progress.emit(50)

          # Paso 4: Traducir texto
          translated_text = translate_text(vocals_text, self.src_language, self.target_language)
          if not translated_text:
              raise Exception("Fallo en la traducción del texto.")
          self.progress.emit(70)

          # Paso 5: Generar audio desde el texto traducido
          output_audio = generate_audio_from_text(translated_text, cleaned_audio_file, self.target_language)
          if not output_audio:
              raise Exception("Fallo en la generación de audio desde el texto.")
          temp_files.append(output_audio)
          self.progress.emit(90)

          # Paso 6: Fusionar video y audio
          final_output = merge_video_audio(self.input_video_path, output_audio)
          if not final_output:
              raise Exception("Fallo en la fusión de video y audio.")
          self.progress.emit(100)

          self.finished.emit(final_output)

      except Exception as e:
          self.error.emit(str(e))

      finally:
          # Limpieza de archivos temporales
          cleanup_temp_files(temp_files)

# Definición de funciones (las mismas que antes, sin cambios significativos)
def extract_audio(input_video_path):
  output_audio_path = "temp_audio.wav"
  try:
      logging.info("Extrayendo audio del video...")
      subprocess.run([
          "ffmpeg", "-y", "-i", input_video_path,
          "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", output_audio_path,
          "-loglevel", "quiet"
      ], check=True)
      return output_audio_path
  except subprocess.CalledProcessError as e:
      logging.error(f"Error al extraer el audio: {e}")
      return None

def remove_background_noise(audio_file):
  try:
      logging.info("Reduciendo el ruido de fondo...")
      audio, sr = sf.read(audio_file)
      # Implementación de reducción de ruido (simplificada)
      reduced_noise_audio = audio  # Aquí podrías aplicar un algoritmo de reducción de ruido
      cleaned_audio_file = "cleaned_audio.wav"
      sf.write(cleaned_audio_file, reduced_noise_audio, sr)
      return cleaned_audio_file
  except Exception as e:
      logging.error(f"Error al reducir el ruido de fondo: {e}")
      return None

def transcribe_audio_whisper(audio_path, src_language):
  try:
      logging.info("Transcribiendo el audio con Whisper...")
      model = whisper.load_model("medium")  # Puedes elegir 'tiny', 'base', 'small', 'medium', o 'large'
      result = model.transcribe(audio_path, language=src_language)
      text = result["text"]
      return text
  except Exception as e:
      logging.error(f"Error en la transcripción con Whisper: {e}")
      return ""

def translate_text(text, src_language, target_language):
  try:
      logging.info("Traduciendo el texto...")
      model_name = "Helsinki-NLP/opus-mt-{}-{}".format(src_language, target_language)
      tokenizer = AutoTokenizer.from_pretrained(model_name)
      model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

      inputs = tokenizer.encode(text, return_tensors="pt", max_length=512, truncation=True)
      outputs = model.generate(inputs, max_length=512, num_beams=4, early_stopping=True)
      translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
      return translated_text
  except Exception as e:
      logging.error(f"Error en la traducción: {e}")
      return ""

def generate_audio_from_text(translated_text, speaker_wav, target_language):
  try:
      logging.info("Generando audio a partir del texto traducido...")
      device = "cuda" if torch.cuda.is_available() else "cpu"
      tts = TTS(model_name="tts_models/multilingual/multi-dataset/your_tts", progress_bar=False, gpu=(device=="cuda"))

      # Configuración de parámetros
      tts_config = {
          "speaker_wav": speaker_wav,
          "language": target_language,
          "file_path": "output_audio.wav"
      }

      tts.tts_to_file(text=translated_text, **tts_config)
      return "output_audio.wav"
  except Exception as e:
      logging.error(f"Error al generar audio desde el texto: {e}")
      return None

def merge_video_audio(input_video_path, input_audio_path):
  output_merged_path = "Final_output_{}.mp4".format(datetime.now().strftime("%Y%m%d_%H%M%S"))
  try:
      logging.info("Fusionando video y audio...")
      subprocess.run([
          "ffmpeg", "-y", "-i", input_video_path, "-i", input_audio_path,
          "-c:v", "copy", "-c:a", "aac", "-map", "0:v:0", "-map", "1:a:0",
          output_merged_path,
          "-loglevel", "quiet"
      ], check=True)
      return output_merged_path
  except subprocess.CalledProcessError as e:
      logging.error(f"Error al fusionar el video y el audio: {e}")
      return None

def cleanup_temp_files(temp_files):
  logging.info("Eliminando archivos temporales...")
  for file in temp_files:
      if os.path.exists(file):
          os.remove(file)
          logging.debug(f"Archivo eliminado: {file}")

# Clase de la ventana principal de la GUI
class VoiceCloneApp(QWidget):
  def __init__(self):
      super().__init__()
      self.setWindowTitle("Clonación y Traducción de Voz en Videos")
      self.setGeometry(100, 100, 500, 200)
      self.init_ui()

  def init_ui(self):
      layout = QVBoxLayout()

      # Selector de archivo de video
      self.video_label = QLabel("Archivo de video:")
      self.video_path_input = QLineEdit()
      self.video_browse_button = QPushButton("Examinar")
      self.video_browse_button.clicked.connect(self.browse_video_file)

      video_layout = QHBoxLayout()
      video_layout.addWidget(self.video_label)
      video_layout.addWidget(self.video_path_input)
      video_layout.addWidget(self.video_browse_button)

      # Selección de idioma fuente
      self.source_language_label = QLabel("Idioma fuente:")
      self.source_language_combo = QComboBox()
      self.source_language_combo.addItems(["es", "en", "fr", "de", "it"])  # Puedes añadir más idiomas

      source_language_layout = QHBoxLayout()
      source_language_layout.addWidget(self.source_language_label)
      source_language_layout.addWidget(self.source_language_combo)

      # Selección de idioma objetivo
      self.target_language_label = QLabel("Idioma objetivo:")
      self.target_language_combo = QComboBox()
      self.target_language_combo.addItems(["en", "es", "fr", "de", "it"])  # Puedes añadir más idiomas

      target_language_layout = QHBoxLayout()
      target_language_layout.addWidget(self.target_language_label)
      target_language_layout.addWidget(self.target_language_combo)

      # Botón de inicio
      self.start_button = QPushButton("Iniciar procesamiento")
      self.start_button.clicked.connect(self.start_processing)

      # Barra de progreso
      self.progress_bar = QProgressBar()
      self.progress_bar.setAlignment(Qt.AlignCenter)

      # Añadir widgets al layout
      layout.addLayout(video_layout)
      layout.addLayout(source_language_layout)
      layout.addLayout(target_language_layout)
      layout.addWidget(self.start_button)
      layout.addWidget(self.progress_bar)

      self.setLayout(layout)

  def browse_video_file(self):
      options = QFileDialog.Options()
      file_name, _ = QFileDialog.getOpenFileName(self, "Seleccionar archivo de video", "", "Videos (*.mp4 *.avi *.mov)", options=options)
      if file_name:
          self.video_path_input.setText(file_name)

  def start_processing(self):
      video_path = self.video_path_input.text()
      src_language = self.source_language_combo.currentText()
      target_language = self.target_language_combo.currentText()

      if not video_path:
          QMessageBox.warning(self, "Advertencia", "Por favor, selecciona un archivo de video.")
          return

      self.thread = VideoProcessingThread(video_path, src_language, target_language)
      self.thread.progress.connect(self.update_progress)
      self.thread.finished.connect(self.processing_finished)
      self.thread.error.connect(self.processing_error)
      self.thread.start()
      self.start_button.setEnabled(False)
      self.progress_bar.setValue(0)

  def update_progress(self, value):
      self.progress_bar.setValue(value)

  def processing_finished(self, output_file):
      self.start_button.setEnabled(True)
      QMessageBox.information(self, "Éxito", f"¡Procesamiento completado!\nArchivo de salida: {output_file}")
      self.progress_bar.setValue(100)

  def processing_error(self, error_message):
      self.start_button.setEnabled(True)
      QMessageBox.critical(self, "Error", f"Ha ocurrido un error:\n{error_message}")
      self.progress_bar.setValue(0)

# Función principal
def main():
  app = QApplication(sys.argv)
  window = VoiceCloneApp()
  window.show()
  sys.exit(app.exec_())

if __name__ == "__main__":
  main()

Explicación de las mejoras y la GUI

    Uso de PyQt5 para la GUI:
        Se ha implementado una interfaz gráfica utilizando la biblioteca PyQt5.
        La interfaz permite seleccionar el archivo de video, elegir el idioma fuente y el idioma objetivo.
        Incluye una barra de progreso que indica el avance del procesamiento.
        Se muestran mensajes de información y de error mediante cuadros de diálogo.

    Procesamiento en segundo plano:
        Se utiliza QThread para ejecutar el procesamiento del video en un hilo separado. Esto asegura que la interfaz gráfica permanezca responsiva durante el procesamiento.
        Las señales (pyqtSignal) se utilizan para actualizar la barra de progreso y manejar la finalización o errores en el procesamiento.

    Amigable y con buena UX:
        La interfaz es sencilla y fácil de usar, evitando la complejidad para el usuario.
        Se proporciona feedback al usuario a través de la barra de progreso y mensajes emergentes.
        Se manejan los errores y se informa adecuadamente al usuario si ocurre algún problema.

    Selección de idiomas:
        Se utilizan QComboBox para permitir al usuario seleccionar los idiomas fuente y objetivo de una lista desplegable.
        Esta lista puede ampliarse añadiendo más códigos de idioma según sea necesario.

    Manejo de dependencias y compatibilidad:
        La GUI se integra con el resto del código sin cambios significativos en la lógica de procesamiento.
        Se han manejado las rutas de archivos y llamadas a funciones para que funcionen correctamente dentro del entorno de PyQt5.

Instrucciones para ejecutar la aplicación
1. Instalación de Python y creación de un entorno virtual (opcional pero recomendado)

Asegúrate de tener Python 3.7 o superior instalado.

Crea un entorno virtual:

bash

Copy Code
python -m venv env

Activa el entorno virtual:

    En Windows:

bash

Copy Code
  env\Scripts\activate

    En macOS/Linux:

bash

Copy Code
  source env/bin/activate

2. Instalación de las dependencias

Crea un archivo requirements.txt con el siguiente contenido:

torch
numpy
pydub
transformers
soundfile
TTS
openai-whisper
PyQt5

Puedes instalar las dependencias ejecutando:

bash

Copy Code
pip install -r requirements.txt

Notas:

    PyTorch: Para aprovechar tu GPU (si tienes una), instala PyTorch con soporte CUDA siguiendo las instrucciones en pytorch.org.

3. Instalación de FFmpeg

La aplicación utiliza FFmpeg para la manipulación de audio y video.

    Windows:
        Descarga FFmpeg desde ffmpeg.org.
        Extrae el contenido y agrega la ruta de la carpeta bin a las variables de entorno del sistema (PATH).

    macOS con Homebrew:

bash

Copy Code
  brew install ffmpeg

    Linux (Ubuntu/Debian):

bash

Copy Code
  sudo apt-get install ffmpeg

4. Ejecutar la aplicación

Guarda el código en un archivo llamado video_voice_clone_gui.py.

Ejecuta la aplicación:

bash

Copy Code
python video_voice_clone_gui.py

5. Uso de la aplicación

    Seleccionar el archivo de video:
        Haz clic en el botón "Examinar" y selecciona el archivo de video que deseas procesar.

    Seleccionar los idiomas:
        Elige el idioma fuente (el idioma en el que está el audio original del video).
        Elige el idioma objetivo (el idioma al que deseas traducir y generar el nuevo audio).

    Iniciar el procesamiento:
        Haz clic en el botón "Iniciar procesamiento".
        La barra de progreso mostrará el avance del procesamiento.
        Una vez finalizado, se te notificará y se indicará la ubicación del archivo de salida.

Notas adicionales

    Idiomas soportados:
        Los idiomas disponibles en los desplegables son: español (es), inglés (en), francés (fr), alemán (de), italiano (it).
        Puedes añadir más idiomas a las listas si lo deseas, asegurándote de que están soportados por Whisper y los modelos de traducción y síntesis.

    Recursos de hardware:
        El procesamiento puede ser intensivo en recursos, especialmente al utilizar modelos más grandes y técnicas de clonación de voz.
        Si experimentas lentitud o limitaciones de memoria, considera utilizar modelos más pequeños o asegurarte de que ninguna otra aplicación está consumiendo muchos recursos.

    Calidad de la clonación de voz:
        La calidad de la clonación de voz puede variar en función de la claridad del audio original y las capacidades del modelo.
        Para obtener los mejores resultados, utiliza videos con audio de alta calidad y minimiza el ruido de fondo.

    Potenciales mejoras:
        Implementar métodos más avanzados de reducción de ruido si es necesario.
        Añadir opciones avanzadas en la interfaz para que el usuario pueda ajustar parámetros como el modelo de Whisper a utilizar, opciones de traducción, etc.
        Internacionalizar la interfaz, ofreciendo traducciones de los textos en la GUI.